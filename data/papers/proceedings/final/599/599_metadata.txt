SubmissionNumber#=%=#599
FinalPaperTitle#=%=#Weighting Finite-State Transductions With Neural Context
ShortPaperTitle#=%=#Weighting Finite-State Transductions With Neural Context
NumberOfPages#=%=#11
CopyrightSigned#=%=#Pushpendre Rastogi
JobTitle#==#
Organization#==#Johns Hopkins University
Abstract#==#How should one apply deep learning to tasks such as
morphological reinflection, which stochastically edit one
string to get another?                          A recent approach to such
sequence-to-sequence tasks is to compress the
input string into a vector that is then used to generate the
output string, using recurrent neural networks.
In contrast, we propose to keep the traditional architecture,
which uses a finite-state transducer to score all possible
output strings, but to augment the scoring function  with the help of
recurrent networks.  A stack of bidirectional LSTMs reads the input string from
left-to-right and right-to-left, in order to summarize the
input context in which a transducer arc is applied.
We combine these learned features with the transducer to define a probability
distribution over aligned output strings, in the form of a weighted
finite-state automaton.
This reduces hand-engineering of features, allows
learned features to examine unbounded context in the input string, and still
permits exact inference through dynamic programming.
We illustrate our method on the
tasks of morphological reinflection and lemmatization.
Author{1}{Firstname}#=%=#Pushpendre
Author{1}{Lastname}#=%=#Rastogi
Author{1}{Email}#=%=#pushpendre@jhu.edu
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#Ryan
Author{2}{Lastname}#=%=#Cotterell
Author{2}{Email}#=%=#ryan.cotterell@gmail.com
Author{2}{Affiliation}#=%=#Johns Hopkins University
Author{3}{Firstname}#=%=#Jason
Author{3}{Lastname}#=%=#Eisner
Author{3}{Email}#=%=#jason@cs.jhu.edu
Author{3}{Affiliation}#=%=#Johns Hopkins University

==========