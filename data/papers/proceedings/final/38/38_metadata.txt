SubmissionNumber#=%=#38
FinalPaperTitle#=%=#Ultradense Word Embeddings by Orthogonal Transformation
ShortPaperTitle#=%=#Ultradense Word Embeddings by Orthogonal Transformation
NumberOfPages#=%=#11
CopyrightSigned#=%=#Sascha Rothe
JobTitle#==#
Organization#==#LMU Munich
Abstract#==#Embeddings are generic representations that are
useful for many NLP tasks. In this paper, we introduce
DENSIFIER, a method that learns an orthogonal transformation
of the embedding space that focuses the information relevant for
a task in an ultradense subspace of a dimensionality
that is smaller by a factor of 100 than the original space.
We show that ultradense embeddings generated by DENSIFIER
reach state of the art on a lexicon creation task in
which words are annotated with three types of lexical
information -- sentiment, concreteness and frequency.
On the SemEval2015 10B sentiment analysis task we show that no information is
lost when the ultradense
subspace is used, but training is an order of magnitude more efficient 
due to the compactness of the ultradense space.
Author{1}{Firstname}#=%=#Sascha
Author{1}{Lastname}#=%=#Rothe
Author{1}{Email}#=%=#sascha@cis.lmu.de
Author{1}{Affiliation}#=%=#LMU Munich
Author{2}{Firstname}#=%=#Sebastian
Author{2}{Lastname}#=%=#Ebert
Author{2}{Email}#=%=#ebert@cis.lmu.de
Author{2}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich
Author{3}{Firstname}#=%=#Hinrich
Author{3}{Lastname}#=%=#Sch√ºtze
Author{3}{Email}#=%=#inquiries@cislmu.org
Author{3}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich

==========