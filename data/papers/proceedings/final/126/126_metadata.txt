SubmissionNumber#=%=#126
FinalPaperTitle#=%=#DAG-Structured Long Short-Term Memory for Semantic Compositionality
ShortPaperTitle#=%=#DAG-Structured Long Short-Term Memory for Semantic Compositionality
NumberOfPages#=%=#10
CopyrightSigned#=%=#Xiaodan Zhu
JobTitle#==#
Organization#==#
Abstract#==#Recurrent neural networks, particularly Long
Short-Term Memory (LSTM), have recently
shown to be very effective in a wide range of
sequence modeling problems, core to which
is effective learning of distributed representation
for sequences or subsequences. A strong
assumption in almost all the previous models,
however, posits that the learned representation
(e.g., the meaning of a sentence), is
fully composable from the atomic components
(e.g., representation for words), while noncompositionality
is a basic phenomenon in human
languages. In this paper, we relieve this
assumption by extending the chain-structured
LSTM to directed acyclic graphs (DAGs),
with the aim to endow linear-chain LSTMs
with the capability of considering both compositionality
and non-compositionality in a semantic
composition framework. Our experiments
on a sentiment composition benchmark
dataset demonstrate that the proposed model
achieves the state-of-the-art performance, significantly
outperforming models that lack this
ability.
Author{1}{Firstname}#=%=#Xiaodan
Author{1}{Lastname}#=%=#Zhu
Author{1}{Email}#=%=#zhu2048@gmail.com
Author{1}{Affiliation}#=%=#National Research Council Canada
Author{2}{Firstname}#=%=#Parinaz
Author{2}{Lastname}#=%=#Sobhani
Author{2}{Email}#=%=#parinaz1366@gmail.com
Author{2}{Affiliation}#=%=#EECS, University of Ottawa
Author{3}{Firstname}#=%=#Hongyu
Author{3}{Lastname}#=%=#Guo
Author{3}{Email}#=%=#hongyu.guo@gmail.com
Author{3}{Affiliation}#=%=#National Research Council Canada

==========