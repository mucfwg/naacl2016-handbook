SubmissionNumber#=%=#521
FinalPaperTitle#=%=#Neural Architectures for Named Entity Recognition
ShortPaperTitle#=%=#Neural Architectures for Named Entity Recognition
NumberOfPages#=%=#11
CopyrightSigned#=%=#Guillaume Lample
JobTitle#==#
Organization#==#Carnegie Mellon University
5000 Forbes Avenue, 15213 Pittsburgh
Abstract#==#State-of-the-art named entity recognition systems rely heavily on hand-crafted
features and domain-specific knowledge in order to learn effectively from the
small, supervised training corpora that are available. In this paper, we
introduce two new neural architectures---one based on bidirectional LSTMs and
conditional random fields, and the other that constructs and labels segments
using a transition-based approach inspired by shift-reduce parsers. Our models
rely on two sources of information about words: character-based word
representations learned from the supervised corpus and unsupervised word
representations learned from unannotated corpora. Our models obtain
state-of-the-art performance in NER in four languages without resorting to any
language-specific knowledge or resources such as gazetteers.
Author{1}{Firstname}#=%=#Guillaume
Author{1}{Lastname}#=%=#Lample
Author{1}{Email}#=%=#guillaume.lample@gmail.com
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Miguel
Author{2}{Lastname}#=%=#Ballesteros
Author{2}{Email}#=%=#miguel.ballesteros@upf.edu
Author{2}{Affiliation}#=%=#Pompeu Fabra University and Carnegie Mellon University
Author{3}{Firstname}#=%=#Sandeep
Author{3}{Lastname}#=%=#Subramanian
Author{3}{Email}#=%=#ssandeep@cs.cmu.edu
Author{3}{Affiliation}#=%=#Language Technologies Institute, Carnegie Mellon University
Author{4}{Firstname}#=%=#Kazuya
Author{4}{Lastname}#=%=#Kawakami
Author{4}{Email}#=%=#www.kazuya.kawakami@gmail.com
Author{4}{Affiliation}#=%=#Carnegie Mellon University
Author{5}{Firstname}#=%=#Chris
Author{5}{Lastname}#=%=#Dyer
Author{5}{Email}#=%=#cdyer@cs.cmu.edu
Author{5}{Affiliation}#=%=#Carnegie Mellon University

==========