SubmissionNumber#=%=#183
FinalPaperTitle#=%=#The Sensitivity of Topic Coherence Evaluation to Topic Cardinality
ShortPaperTitle#=%=#The Sensitivity of Topic Coherence Evaluation to Topic Cardinality
NumberOfPages#=%=#5
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#When evaluating the quality of topics generated by a topic model, the
convention is to score topic coherence — either manually or automatically —
using the top-N topic words. This hyper-parameter N , or the cardinality of the
topic, is often overlooked and selected arbitrarily. In this paper, we
investigate the impact of this cardinality hyper-parameter on topic coherence
evaluation. For two automatic topic coherence methodologies, we observe that
the correlation with human ratings decreases systematically as the cardinality
increases. More interestingly, we find that performance can be improved if the
system scores and human ratings are aggregated over several topic cardinalities
before computing the correlation. In contrast to the standard practice of using
a fixed value of N (e.g. N = 5 or N = 10), our results suggest that calculating
topic coherence over several different cardinalities and averaging results in a
substantially more stable and robust evaluation. We release the code and the
datasets used in this research, for reproducibility.
Author{1}{Firstname}#=%=#Jey Han
Author{1}{Lastname}#=%=#Lau
Author{1}{Email}#=%=#jeyhan.lau@gmail.com
Author{1}{Affiliation}#=%=#IBM Research
Author{2}{Firstname}#=%=#Timothy
Author{2}{Lastname}#=%=#Baldwin
Author{2}{Email}#=%=#tb@ldwin.net
Author{2}{Affiliation}#=%=#The University of Melbourne

==========