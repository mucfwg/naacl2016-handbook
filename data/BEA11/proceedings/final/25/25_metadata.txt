SubmissionNumber#=%=#25
FinalPaperTitle#=%=#Automated scoring across different modalities
ShortPaperTitle#=%=#Automated scoring across different modalities
NumberOfPages#=%=#6
CopyrightSigned#=%=#Anastassia Loukina
JobTitle#==#
Organization#==#
Abstract#==#In this paper we investigate how well the systems developed for automated
evaluation of written responses perform when applied to spoken responses. We
compare two state of the art systems for automated writing evaluation and the
state of the art system for evaluating spoken responses. We find that the
systems for writing evaluation achieve very good performance when applied to
transcriptions of spoken responses but show degradation when applied to ASR
output. The system based on sparse $n$-gram features appears to be more robust
to such degradation. We further explore the role of ASR accuracy and the
performance and construct coverage of the combined model which includes all
three engines.
Author{1}{Firstname}#=%=#Anastassia
Author{1}{Lastname}#=%=#Loukina
Author{1}{Email}#=%=#aloukina@ets.org
Author{1}{Affiliation}#=%=#Educational Testing Service
Author{2}{Firstname}#=%=#Aoife
Author{2}{Lastname}#=%=#Cahill
Author{2}{Email}#=%=#acahill@ets.org
Author{2}{Affiliation}#=%=#Educational Testing Service

==========