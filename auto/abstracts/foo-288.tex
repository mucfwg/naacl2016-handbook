For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages. We experiment with extensions of the neural, attentional model of Bahdanau et al. for such data. On phone-to-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++'s performance on gold transcriptions, but without any recourse to transcriptions or to a lexicon.
