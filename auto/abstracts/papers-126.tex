Recurrent neural networks, particularly Long Short-Term Memory (LSTM), have recently shown to be very effective in a wide range of sequence modeling problems, core to which is effective learning of distributed representation for sequences or subsequences. A strong assumption in almost all the previous models, however, posits that the learned representation (e.g., the meaning of a sentence), is fully composable from the atomic components (e.g., representation for words), while noncompositionality is a basic phenomenon in human languages. In this paper, we relieve this assumption by extending the chain-structured LSTM to directed acyclic graphs (DAGs), with the aim to endow linear-chain LSTMs with the capability of considering both compositionality and non-compositionality in a semantic composition framework. Our experiments on a sentiment composition benchmark dataset demonstrate that the proposed model achieves the state-of-the-art performance, significantly outperforming models that lack this ability.
