This paper proposes an incremental learning strategy of neural word embedding methods, which is a simple modification of conventional methods, such as SkipGrams and Global Vectors. Since our method iteratively constructs embedding vectors one dimension by one dimension, obtained vectors equip a unique property, namely, any right-truncated vector matches the solutions of the corresponding lower-dimensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. We demonstrate the effectiveness of our method by evaluating the obtained embedding vectors on a wide variety of linguistic benchmark data.
