In this paper we investigate how well the systems developed for automated evaluation of written responses perform when applied to spoken responses. We compare two state of the art systems for automated writing evaluation and the state of the art system for evaluating spoken responses. We find that the systems for writing evaluation achieve very good performance when applied to transcriptions of spoken responses but show degradation when applied to ASR output. The system based on sparse \$n\$-gram features appears to be more robust to such degradation. We further explore the role of ASR accuracy and the performance and construct coverage of the combined model which includes all three engines.
