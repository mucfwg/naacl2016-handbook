Automatic Machine Translation metrics, such as BLEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation over that of a baseline, it is important to take into account the uncertainty inherent in correlation point estimates prior to concluding improvements in metric performance. Confidence intervals for correlations with human judgment are rarely reported in metric evaluations, however, and when they have been reported, appropriate methods have unfortunately not been applied. Several issues including incorrect assumptions about correlation sampling distributions greatly risk the over-estimation of significant differences in metric performance. In this paper, we provide analysis of each of the issues leading to inaccuracies in current evaluations before providing detail of an appropriate method that overcomes previous challenges. Additionally, we propose a new method of translation sampling that in contrast achieves genuine high conclusivity in evaluation of the relative performance of metrics.
