Bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) has been successfully applied in many tagging tasks and shows state-of-the-art performance. To involve word-level information, BLSTM-RNN uses the distributed representations of words. Thus training better distributed word representations is an alternative way to improve the performance of BLSTM-RNN on tagging tasks aside refining model. In this work, we propose a novel approach to learn distributed word representations with BLSTM-RNN. Experimental results show that our approach learns high qualified distributed word representations, as the trained representations significantly elevate the performance of BLSTM-RNN on three tagging tasks: part-of-speech tagging, chunking and named entity recognition, surpassing word representations trained by other published methods.
