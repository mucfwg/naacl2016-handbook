In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require on any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of single-word compounds (like ``candle'' and ``stick'') are semantically similar to the entire compound, which helps to exclude spurious splits (like ``candles'' ``tick''). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter.
